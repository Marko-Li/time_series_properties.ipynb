#### Part I: Time Series Acquisition -- data source 

### Code below is to retrieve tracks data from plb library
import numpy as np
import pandas as pd
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Ridge,Lasso, LassoCV

## x_feature depends on the variables you aim to analyze
x_feature = ['SX5T','NKYNTR','NDUEACWF','NDDUE15','NDDUNA','XCMP','SPTR500N','SPG1200T','EDE15BV','SXXR','SXGBMV']


benchmark_data = plb.universe.code(x_feature[0]).tracks(True).to_frame()
## store eleven sets of benchmark data by outer join
for i in range(1,11):
    temp = plb.universe.code(x_feature[i]).tracks(True).to_frame()
    benchmark_data = benchmark_data.join(temp, how='outer')
benchmark_data = benchmark_data.dropna()

#################################
## 3780 sets of 6056 data in total is doable

import sweetviz as sv
data_report = sv.analyze(benchmark_data)
data_report.show_html()

##################################

## generate weights of 'assets' following normal distribution and do multiple factors linear regression

a = np.random.normal(0,1,11)
a /= a.sum()
for i in range(10):
    df.iloc[:,i] = benchmark_data.iloc[:,i]*a[i]
df = df.dropna()
portfolio = df.sum(axis = 1)
data_x = benchmark_data[x_feature]
data_y = portfolio
model = sm.regression.linear_model.OLS(data_y, data_x)
reg = model.fit()
reg.summary()

##################################

#### Part II: Time Series Properties

Section I: Correlation Matrix of returns

## calculte daily/ monthly percentage change(return) 

daily_returns = benchmark_data.pct_change().dropna() ## daily returns
monthly_returns = benchmark_data.resample('M').ffill().pct_change().dropna() ## weekly returns

### Import package and change settings

import matplotlib as mpl
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
import seaborn as sns
import datetime
# import jieba
import sweetviz as sv
import statsmodels.api as sm
# from wordcloud import WordCloud, STOPWORDS
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Ridge,Lasso, LassoCV
import featuretools as ft

mpl.rcParams['font.sans-serif'] = ['simhei'] ## Using letters by default  
mpl.rcParams['axes.unicode_minus'] = False ## It can solve the problem that minus looks like square in visualization
%matplotlib inline 


### Correlation Matrix for daily_returns
data_correlation = daily_returns[x_feature]
corr = round(data_correlation.corr(), 3)

sns.set(style = 'darkgrid', rc = {"figure.figsize": (10, 5)}, font_scale = 3.0)  
# Point out the size of letters and patterns

mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

f, ax = plt.subplots(figsize=(33, 27))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True)
plt.show()

### Correlation Matrix for monthly_returns

data_correlation = monthly_returns[x_feature]
corr = round(data_correlation.corr(), 3)

sns.set(style = 'darkgrid', rc = {"figure.figsize": (10, 5)}, font_scale = 3.0)  
# Point out the size of letters and patterns

mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

f, ax = plt.subplots(figsize=(33, 27))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True)
plt.show()

############################

Section II: Annual Volatility

## In this section, we analyze the annual volatility and fulfill visualization automatically by given x_feature (asset/stock name)

sns.set(style = 'darkgrid', rc = {"figure.figsize": (10, 5)}, font_scale = 1.0)  
import matplotlib.pyplot as plt

x_feature = ['SX5T','NKYNTR','NDUEACWF','NDDUE15','NDDUNA','XCMP','SPTR500N','SPG1200T','EDE15BV','SXXR','SXGBMV']
annual_volatility = pd.DataFrame(columns=x_feature)

for i in range (len(x_feature)):
    temp = plb.universe.code(x_feature[i]).tracks(True).to_frame()
    temp['Log returns'] = np.log(temp[x_feature[i]]/temp[x_feature[i]].shift())
    temp = temp.dropna()
    temp['Log returns'].std()
    volatility = temp['Log returns'].std()*252**.5
    
    ## record the value of annual volatility for different assets in the dataframe annual_volatility
    annual_volatility.at['annual_volatility', x_feature[i]] = round(volatility, 4)
    
    ## Fulfill visualization of historical data for every stock in x_feature
    
    str_vol = str(round(volatility, 4)*100)
    fig, ax = plt.subplots()
    temp['Log returns'].hist(ax=ax, bins=50, alpha=0.6, color='b')
    ax.set_xlabel('Log return')
    ax.set_ylabel('Freq of log return')
    ax.set_title(str(x_feature[i]) + ' annual volatility: ' + str_vol + '%')

################################

Section III: Outliers

## write a loop to filter outlier by 99 percentage quantile and then store filtering sets and outlier sets separately

data_filter = benchmark_data
outlier = pd.DataFrame()
for i in range (len(x_feature)):
    upper_bound = benchmark_data.quantile([0.99])[x_feature[i]]
    lower_bound = benchmark_data.quantile([0.01])[x_feature[i]]
    data_filter = data_filter[(data_filter[x_feature[i]] > int(lower_bound)) & (data_filter[x_feature[i]] < int(upper_bound))]
    temp = benchmark_data[(benchmark_data[x_feature[i]] < int(lower_bound)) | (benchmark_data[x_feature[i]] > int(upper_bound))]
    outlier = pd.concat([outlier, temp], axis = 0)
outlier = outlier.drop_duplicates()


################################

Section IV: Collinearity (VIF)

## Analyse the collinearity among time series with VIF

from patsy import dmatrices
from statsmodels.stats.outliers_influence import variance_inflation_factor

#find design matrix for linear regression model using portfolio as response variable 
## benchmark_data could be some datasets of tracks for different x_feature

features = "+".join(benchmark_data[x_feature].columns) 
y, X = dmatrices('portfolio ~' + features, data=benchmark_data[x_feature], return_type='dataframe')

#calculate VIF for each explanatory variable
vif = pd.DataFrame()
vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
vif['variable'] = X.columns
vif

################################

Section V: write build_portfolio function to give tracks of portfolio
   
## Use tracks, variables name and proportion to be parameter in the function of building portfolio

def build_portfolio(x_feature, tracks, proportion):
    stock_data = tracks[x_feature]
    temp = pd.DataFrame(columns=x_feature)
    for i in range(len(x_feature)):
        temp.iloc[:,i] = stock_data.iloc[:,i]*proportion[i]
    portfolio = temp.sum(axis = 1)
    return portfolio


## one example to use this function

variables = ['NKYNTR','NDUEACWF','NDDUE15','NDDUNA','XCMP','SPTR500N','SPG1200T','EDE15BV','SXXR','SXGBMV']
data = benchmark_data
proportions = [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1]
build_portfolio(variables, data, proportions)

################################
